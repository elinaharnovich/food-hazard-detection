{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "deb53424-5e8a-4de2-895b-aa88a13b0bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-30 16:09:32--  https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3832 (3.7K) [text/plain]\n",
      "Saving to: ‘minsearch.py.1’\n",
      "\n",
      "minsearch.py.1      100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-08-30 16:09:32 (44.5 MB/s) - ‘minsearch.py.1’ saved [3832/3832]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f35b1666-5393-458a-b747-177c1a1525ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f98df4-326c-4a0f-95a3-2441e1ced3ca",
   "metadata": {},
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "acee6281-a506-4dc0-9f82-8b2ad48346ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/incidents_train.csv\", index_col=False, dtype=str)\n",
    "df = df.rename(columns={\"Unnamed: 0\": \"id\", \"hazard-category\": \"hazard_category\", \"product-category\": \"product_category\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "92bdbd8d-eef7-4f2a-a55b-37e7c17f17b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b60b7c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only documents that ground truth been created for\n",
    "documents = documents[1013:1479]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3cb61ed6-815f-47a6-8e5d-39ddca63982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "267f9429-4f31-4750-bdf4-84d0a3ec554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = minsearch.Index(\n",
    "    #text_fields=['id', 'year', 'month', 'day', 'country', 'title', 'text', 'hazard_category', 'product_category', 'hazard', 'product'],\n",
    "    #text_fields=['country', 'title', 'text', 'hazard_category', 'product_category', 'hazard', 'product'],\n",
    "    text_fields=['title', 'hazard_category', 'product_category', 'hazard', 'product'],\n",
    "    keyword_fields=['id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d734c998-f4a5-4390-9708-6c1edda23348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7f30dc046e30>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70bc03-575d-4576-8b1d-f82462feb485",
   "metadata": {},
   "source": [
    "## RAG flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0eef0c9-3e59-4b02-9f49-57f7ae78e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1/',\n",
    "    api_key='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "96687ff0-3348-4743-a56a-786127c84fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2450e48b-b4e3-4555-9e01-5674c3ff0404",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a food hazard detection assistant. Answer the QUESTION based on the CONTEXT from the food-incident reports.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "entry_template = \"\"\"\n",
    " 'title': {title}\n",
    " 'hazard_category': {hazard_category}\n",
    " 'product_category': {product_category}\n",
    " 'hazard': {hazard}\n",
    " 'product': {product}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + entry_template.format(**doc) + \"\\n\\n\"\n",
    "\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3881e794-a85b-4bd2-a9a0-ae5ab1021776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, model='llama3.1'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=-0.2\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d1d1c70-a21d-4af7-8168-22215303687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, model='llama3.1'):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    #print(prompt)\n",
    "    answer = llm(prompt, model=model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c77a0a9e-3955-405f-8bff-66214f79e6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, there are no specific reports of listeria monocytogenes being detected in \"Double Cream\". However, there is a mention of a recall for other dairy products.\n",
      "\n",
      "There are several mentions of listeria monocytogenes with different product categories:\n",
      "\n",
      "* Chicken based products (4 separate recalls)\n",
      "* Precooked beef meat products\n",
      "* Pig meat - pork\n",
      "* Raw goat milk cheese\n",
      "\n",
      "Unfortunately, none of the provided reports include specific information about products that contain listeria monocytogenes.\n"
     ]
    }
   ],
   "source": [
    "question = 'What are the products that contain listeria monocytogenes?'\n",
    "answer = rag(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9100bd80-431d-4990-98f7-0ef5ed0fd174",
   "metadata": {},
   "source": [
    "## Retrieval evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a88433ad-dcc1-4455-a0df-3006b43c52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_question = pd.read_csv('../data/ground-truth-retrieval.tsv', delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f6525af2-4c66-4f4a-88c2-91e0b6b8dc59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1013</td>\n",
       "      <td>What specific batches of vacuum-packed Organic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1013</td>\n",
       "      <td>Are there any reported cases of botulism assoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1013</td>\n",
       "      <td>How did the company's poor or insufficient con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1013</td>\n",
       "      <td>Has the relevant regulatory agency investigate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1013</td>\n",
       "      <td>What steps does The Engine Shed plan to take t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                           question\n",
       "0  1013  What specific batches of vacuum-packed Organic...\n",
       "1  1013  Are there any reported cases of botulism assoc...\n",
       "2  1013  How did the company's poor or insufficient con...\n",
       "3  1013  Has the relevant regulatory agency investigate...\n",
       "4  1013  What steps does The Engine Shed plan to take t..."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_question.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e41a0aa-8f78-4aeb-b334-dd963d63a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = df_question.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c86b03ae-5a68-4ac4-9314-00b521b56272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1013,\n",
       " 'question': 'What specific batches of vacuum-packed Organic Tofu were recalled by The Engine Shed?'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ad9e893-c880-4af5-b6df-4a9b2806d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7d576457-2365-49bc-a573-df306e999c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_search(query):\n",
    "    boost = {}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "49033d89-9f85-4374-a379-28b4dd9ec14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bde9bbdd-f1a3-4e73-8538-d2591ed159c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['id']\n",
    "        results = search_function(q)\n",
    "        relevance = [int(d['id']) == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d2dd4e92-ca43-4b0c-ae47-6ec89b83a622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade4b788a3ad4e57a5faaedc94edaf22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.6124197002141327, 'mrr': 0.41243992386390693}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ground_truth, lambda q: minsearch_search(q['question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95137144-7060-4e4c-aba5-359de3085144",
   "metadata": {},
   "source": [
    "## Finding the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "387e641c-6592-4e4f-b191-1e5ca7598f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = df_question[:100]\n",
    "df_test = df_question[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1a95c044-df61-4cb6-84a9-19ff9e1ef107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def simple_optimize(param_ranges, objective_function, n_iterations=10):\n",
    "    best_params = None\n",
    "    best_score = float('-inf')  # Assuming we're minimizing. Use float('-inf') if maximizing.\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # Generate random parameters\n",
    "        current_params = {}\n",
    "        for param, (min_val, max_val) in param_ranges.items():\n",
    "            if isinstance(min_val, int) and isinstance(max_val, int):\n",
    "                current_params[param] = random.randint(min_val, max_val)\n",
    "            else:\n",
    "                current_params[param] = random.uniform(min_val, max_val)\n",
    "        \n",
    "        # Evaluate the objective function\n",
    "        current_score = objective_function(current_params)\n",
    "        \n",
    "        # Update best if current is better\n",
    "        if current_score > best_score:  # Change to > if maximizing\n",
    "            best_score = current_score\n",
    "            best_params = current_params\n",
    "    \n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6535225c-aa11-4875-895e-f84f2020083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_val = df_validation.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "33d2c962-b56a-4f1d-892a-1b84e89f4f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_search(query, boost=None):\n",
    "    if boost is None:\n",
    "        boost = {}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9d61d688-45a0-40c9-bf5a-e87ad778ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_ranges = {\n",
    "    'title': (0.0, 3.0),\n",
    "    'hazard_category': (0.0, 3.0),\n",
    "    'product_category': (0.0, 3.0),\n",
    "    'hazard': (0.0, 3.0),\n",
    "    'product': (0.0, 3.0)\n",
    "}\n",
    "\n",
    "def objective(boost_params):\n",
    "    def search_function(q):\n",
    "        return minsearch_search(q['question'], boost_params)\n",
    "\n",
    "    results = evaluate(gt_val, search_function)\n",
    "    return results['mrr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "85f903c6-9c07-4399-9cca-e12bda23e7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07eaa76556754b1b8d27f42ec8b33b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad97f9bb93d47089071663c371c7ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb666cdeb18c4d78901b333fc2f67863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb9b38db0f440abaa34438440c63f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d3f6856db14b6f8c9a227b0aa44c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0677b60ed54b0cb59f360f1f886d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9f2d3e7bb84b569aa2c1431fee3454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee68a3ae84248f0a8ea6441c64b53ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1987b9673d44dcad7ebf29e23e207a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddc8211f962401aa8ee30423bdbb93a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e822207dbbfb43d198caeed4df2cd73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c367d32408f4afeb1dc71092fc948b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c486ca0c2a4223a2da196c8ee5868b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f961f2cb4b747aa9c0c1f4f400f6e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e37a2fd289848d5aca693a6186c44b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d763e411f7405b8d0337877dc7adf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37f9e08fd6144538ace91755163908c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd238b117e0467897e85b5ef84978a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ac745ff06f54346bf4e2380f2de1d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d162c9c0c71e42a8b1f688ee3d42228b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "({'title': 2.9956641322868047,\n",
       "  'hazard_category': 0.41403862613871456,\n",
       "  'product_category': 1.9716156627484709,\n",
       "  'hazard': 0.9218557425650392,\n",
       "  'product': 0.6469289358889216},\n",
       " 0.582)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_optimize(param_ranges, objective, n_iterations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7e11a1b9-9375-4c24-bfb3-0170ce82187d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6283e77f384663b2bb9efabae4c81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.6672376873661671, 'mrr': 0.47780649875938963}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def minsearch_improved(query):\n",
    "    boost = {\n",
    "        'title': 2.99,\n",
    "        'hazard_category': 0.41,\n",
    "        'product_category': 1.97,\n",
    "        'hazard': 0.92,\n",
    "        'product': 0.64\n",
    "    }\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "evaluate(ground_truth, lambda q: minsearch_improved(q['question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de669d5-f9a4-4da0-b533-2781204ece3b",
   "metadata": {},
   "source": [
    "## RAG evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4ab3ae9a-8639-4244-ada3-90d321f0ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bb3a0f2a-a232-4864-bf43-c93578667914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2335"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e5d7bc5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1013,\n",
       " 'question': 'What specific batches of vacuum-packed Organic Tofu were recalled by The Engine Shed?'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b2be59fd-2d38-4aeb-a335-025dc1cab029",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = ground_truth[0]\n",
    "question = record['question']\n",
    "answer_llm = rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8f1a5c96-6904-4cd6-8f36-4c6f56ead161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of the recalled products are vacuum-packed Organic Tofu. However, there is a mention of \"The Engine Shed recalls various batches of their vacuum-packed Organic Tofu due to a potential risk of botulism\" in the context provided.\n",
      "\n",
      "Based on this information, I'm unable to provide specific batch numbers for these recalled Organic Tofus as that particular product's recall is only mentioned briefly and not detailed further.\n"
     ]
    }
   ],
   "source": [
    "print(answer_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "311e3d42-30a4-4fe7-906c-c3806442bc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert evaluator for a RAG system.\n",
      "Your task is to analyze the relevance of the generated answer to the given question.\n",
      "Based on the relevance of the generated answer, you will classify it\n",
      "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
      "\n",
      "Here is the data for evaluation:\n",
      "\n",
      "Question: What specific batches of vacuum-packed Organic Tofu were recalled by The Engine Shed?\n",
      "Generated Answer: None of the recalled products are vacuum-packed Organic Tofu. However, there is a mention of \"The Engine Shed recalls various batches of their vacuum-packed Organic Tofu due to a potential risk of botulism\" in the context provided.\n",
      "\n",
      "Based on this information, I'm unable to provide specific batch numbers for these recalled Organic Tofus as that particular product's recall is only mentioned briefly and not detailed further.\n",
      "\n",
      "Please analyze the content and context of the generated answer in relation to the question\n",
      "and provide your evaluation in parsable JSON without using code blocks:\n",
      "\n",
      "{\n",
      "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
      "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt2_template.format(question=question, answer_llm=answer_llm)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c081e711-4e5c-4cd4-bc8d-79a29c1f1d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6da1bf76-7670-4723-847d-15aad4a5d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_question.sample(n=200, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "98a38031-1e2b-4134-bc88-6d9e857771f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df_sample.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4fa05926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1319,\n",
       " 'question': 'How will consumers be notified about this recall, and what steps should they take to return the affected products?'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3b8e5061",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66cdc62-803c-4c79-a687-581788fe1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in tqdm(sample):\n",
    "    id = record['id']\n",
    "\n",
    "    if id in evaluations:\n",
    "        continue\n",
    "    \n",
    "    question = record['question']\n",
    "    answer_llm = rag(question) \n",
    "\n",
    "    prompt = prompt2_template.format(\n",
    "        question=question,\n",
    "        answer_llm=answer_llm\n",
    "    )\n",
    "\n",
    "    evaluation = llm(prompt)\n",
    "    print(evaluation)\n",
    "    if \"```\" in evaluation:\n",
    "        evaluation = evaluation.split(\"```\")[1].lstrip(\"json\")\n",
    "    elif \"\\n\\n\" in evaluation:\n",
    "        evaluation = evaluation.split(\"\\n\\n\")[1]\n",
    "    evaluation = json.loads(evaluation)\n",
    "\n",
    "    evaluations[id] = {\n",
    "        'id': id,\n",
    "        'question': question,\n",
    "        'answer_llm': answer_llm,\n",
    "        'evaluation': evaluation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "638b2f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1319,\n",
       " 'question': 'How will consumers be notified about this recall, and what steps should they take to return the affected products?',\n",
       " 'answer_llm': \"Based on the context provided, it appears that there are multiple recalls of beef products due to misbranding and undeclared allergens. However, I will focus on the specific recall mentioned in the context:\\n\\n**California Firm Recalls Beef Products Due to Misbranding and Undeclared Allergen**\\n\\n**Hazard:** Milk and products thereof\\n\\n**Product:** Beef products\\n\\nTo answer your question:\\n\\nConsumers will be notified about this recall through a public notice, likely posted on the website of the relevant regulatory agency (such as the FDA or USDA). The notice may also be disseminated through other channels, such as social media, news outlets, and point-of-sale notifications.\\n\\nAs for what steps consumers should take to return the affected products:\\n\\nConsumers who have purchased the recalled beef products should check their packaging for the specific product name and code. If they find that their product is among those recalled, they should:\\n\\n1. Stop consuming the product immediately.\\n2. Return the product to the place of purchase or contact the recalling firm directly (in this case, the California Firm) for instructions on how to return the product.\\n\\nIt's essential to note that specific return procedures may vary depending on the recalling firm and local regulations. Consumers should follow the instructions provided by the recalling firm or regulatory agency for proper disposal or return of the affected products.\",\n",
       " 'evaluation': {'Relevance': 'RELEVANT',\n",
       "  'Explanation': 'The generated answer directly addresses the question by providing information on how consumers will be notified about the recall and what steps they should take to return the affected products. The answer is specific, clear, and concise in its response.'}}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations[1319]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "db471168",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(evaluations, columns=['id', 'question', 'relevance', 'answer', 'explanation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f7650eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in evaluations.items():\n",
    "    new_entry = {\n",
    "        'id': v['id'],\n",
    "        'question': v['question'],\n",
    "        'relevance': v['evaluation']['Relevance'],\n",
    "        'answer': v['answer_llm'],\n",
    "        'explanation': v['evaluation']['Explanation']\n",
    "    }\n",
    "    df1 = pd.DataFrame(new_entry, index=[0])\n",
    "    df_eval = pd.concat([df_eval, df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cf61170e-bfde-4889-8cd3-98fbfcc89918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_eval = pd.DataFrame(evaluations, columns=['record', 'answer', 'evaluation'])\n",
    "\n",
    "# df_eval['id'] = df_eval.record.apply(lambda d: d['id'])\n",
    "# df_eval['question'] = df_eval.record.apply(lambda d: d['question'])\n",
    "\n",
    "# df_eval['relevance'] = df_eval.evaluation.apply(lambda d: d['Relevance'])\n",
    "# df_eval['explanation'] = df_eval.evaluation.apply(lambda d: d['Explanation'])\n",
    "\n",
    "# del df_eval['record']\n",
    "# del df_eval['evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e30bcd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6a9b40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv('../data/rag-evaluationa-llama3.1.tsv', sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
